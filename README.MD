In order to quantify the quality of information contained in hidden layers, I experimented with how well a logistic regression function could classify images using only the output from each hidden layer of the VGG19 network. I used a VGG19 network with pre-trained ImageNet weights, as well as a VGG19 network with random initial weights (for control, as well as to see how the tests fared with a network not trained on the ImageNet data, which was used for testing). For our training data, I combined 250 images each from the Tiny ImageNet database in the following categories: goldfish, mushroom, alpine, golden retriever, baboon, Egyptian cat, trilobite, snail, and CD player. I also added 250 more random images to the training data. For each layer and for each category, a logistic regression model was fit to predict whether an image was in said category. (The logistic regression model itself had to be trained, but had to rely heavily on the layers for performing classification.) For validation, I combined 50 images from each category as well as 800 other images from Tiny ImageNet. I then tested the accuracy of the logistic regression function to classify the images of the validation set.

Results:

![chart](https://github.com/maxlevinestuff/VGG19-Feature-Extraction/blob/main/imagenet.png?raw=true)